<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
  </style>
</head>
<body>
<style>
@media print {

  .code {
    -webkit-print-color-adjust:exact;
  }

}
</style>
<h1 id="assignment-5">Assignment 5</h1>
<h2 id="statistical-modelling-i">Statistical Modelling I</h2>
<h3 id="clinton-b-morris-mauricio-garcia-tec">Clinton B Morris <br> Mauricio Garcia Tec</h3>
<h4 id="october-2017">October 2017</h4>
<p>We load the libraries we will use</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(glmnet) <span class="co"># Library for penalized regression models and cross-validation </span>
<span class="kw">library</span>(caret) <span class="co"># General Purpose Machine Learning Modelling Framework</span>
<span class="kw">library</span>(tidyverse) <span class="co"># Efficient data manipulation and I/O</span>
<span class="kw">library</span>(ggplot2) <span class="co"># A grammar of graphics for plotting</span>
<span class="kw">library</span>(ggthemes) <span class="co"># Improves layout adn colors for ggplot2 graphics</span></code></pre></div>
<h3 id="the-task">The Task</h3>
<p>We obtain the data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">concrete &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/SDS383team/HW5/master/data/ConcreteData.csv&quot;</span>)</code></pre></div>
<p>Here is how the first rows look like:</p>
<table>
<thead>
<tr class="header">
<th align="right">Cement</th>
<th align="right">Blast Furnace Slag</th>
<th align="right">Fly Ash</th>
<th align="right">Water</th>
<th align="right">Superplasticizer</th>
<th align="right">Coarse Aggregate</th>
<th align="right">Fine Aggregate</th>
<th align="right">Age</th>
<th align="right">Compressive strength</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">540.0</td>
<td align="right">0.0</td>
<td align="right">0</td>
<td align="right">162</td>
<td align="right">2.5</td>
<td align="right">1040.0</td>
<td align="right">676.0</td>
<td align="right">28</td>
<td align="right">79.99</td>
</tr>
<tr class="even">
<td align="right">540.0</td>
<td align="right">0.0</td>
<td align="right">0</td>
<td align="right">162</td>
<td align="right">2.5</td>
<td align="right">1055.0</td>
<td align="right">676.0</td>
<td align="right">28</td>
<td align="right">61.89</td>
</tr>
<tr class="odd">
<td align="right">332.5</td>
<td align="right">142.5</td>
<td align="right">0</td>
<td align="right">228</td>
<td align="right">0.0</td>
<td align="right">932.0</td>
<td align="right">594.0</td>
<td align="right">270</td>
<td align="right">40.27</td>
</tr>
<tr class="even">
<td align="right">332.5</td>
<td align="right">142.5</td>
<td align="right">0</td>
<td align="right">228</td>
<td align="right">0.0</td>
<td align="right">932.0</td>
<td align="right">594.0</td>
<td align="right">365</td>
<td align="right">41.05</td>
</tr>
<tr class="odd">
<td align="right">198.6</td>
<td align="right">132.4</td>
<td align="right">0</td>
<td align="right">192</td>
<td align="right">0.0</td>
<td align="right">978.4</td>
<td align="right">825.5</td>
<td align="right">360</td>
<td align="right">44.30</td>
</tr>
</tbody>
</table>
<p>Our <strong>task</strong> is to compare regression models where the response variable is <strong>Compressive strength</strong> and the rest are used as predictors.</p>
<p>To begin, we first create train and test sets splitting the data by half, then construct we define the design matrix <span class="math inline"><em>X</em></span> and response vector <span class="math inline"><em>y</em></span> for each testing and training sets. The train and test data will be stored in matrix and numeric vector form since the package <code>glmnet</code> which we use later need an input of this form.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">999</span>) <span class="co"># for reproducibility</span>
train_idx &lt;-<span class="st">  </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(concrete), <span class="dt">size =</span> <span class="kw">nrow</span>(concrete) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)
test_data &lt;-<span class="st"> </span>concrete <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(train_idx)
train_data &lt;-<span class="st"> </span>concrete <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="op">-</span>train_idx)
X_test &lt;-<span class="st"> </span>test_data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="st">`</span><span class="dt">Compressive strength</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">data.matrix</span>()
X_train &lt;-<span class="st"> </span>train_data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="st">`</span><span class="dt">Compressive strength</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">data.matrix</span>()
y_test &lt;-<span class="st"> </span>test_data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(<span class="st">`</span><span class="dt">Compressive strength</span><span class="st">`</span>) 
y_train &lt;-<span class="st"> </span>train_data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(<span class="st">`</span><span class="dt">Compressive strength</span><span class="st">`</span>)</code></pre></div>
<h3 id="running-the-models">Running the Models</h3>
<p>Below are the commands to compute five different regression models and estimate their generalization errors.</p>
<h4 id="multiple-linear-regression">(1) Multiple Linear Regression</h4>
<p>We run and save the RMSE for linear regression on the full training with all the variables using <code>lm.fit</code>–which is the core of the base <code>lm</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mlr &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Compressive strength</span><span class="st">`</span> <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train_data)
rmse_train_mlr &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((y_train <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(mlr, train_data))<span class="op">^</span><span class="dv">2</span>))
rmse_test_mlr &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((y_test <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(mlr, test_data))<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<p>We now repeat the same training but with <strong>10-fold cross validation</strong> on the training set. Since we are not tuning any parameter the average RMSE should be very similar to the rmse on the test set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Number of folds for cross-validation</span>
nfolds &lt;-<span class="st"> </span><span class="dv">10</span>
size &lt;-<span class="st"> </span><span class="kw">nrow</span>(train_data)
<span class="co"># Create 10 equal size folds as a list of validation indexes</span>
folds &lt;-<span class="st"> </span><span class="kw">split</span>(<span class="kw">sample</span>(size), <span class="kw">cut</span>(<span class="dv">1</span><span class="op">:</span>size, <span class="dt">breaks =</span> nfolds, <span class="dt">labels =</span> <span class="ot">FALSE</span>))
<span class="co"># Perform 10 fold cross validation</span>
rmse_cv_mlr_k &lt;-<span class="st"> </span><span class="kw">numeric</span>(nfolds)
<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nfolds) {
  idx &lt;-<span class="st"> </span>folds[[k]]
  mod &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Compressive strength</span><span class="st">`</span> <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train_data[<span class="op">-</span>idx, ])
  rmse_cv_mlr_k[k] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((y_train[idx] <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(mod, train_data[idx, ]))<span class="op">^</span><span class="dv">2</span>))
}
rmse_cv_mlr &lt;-<span class="st"> </span><span class="kw">mean</span>(rmse_cv_mlr_k)</code></pre></div>
<h4 id="multiple-linear-regression-model-selection-with-bic">(2) Multiple Linear Regression &amp; Model Selection with BIC</h4>
<p>We will now choose the best model using the Bayesian Information Criterion. The BIC is a function that penalizes each free variable, selecting the one that best minimizes <span class="math inline">$p\log(n) - 2\log(\hat{L})$</span> where <span class="math inline"><em>n</em></span> is the number of observations on the dataset. The functionality of model selection using BIC in R is given by the function <code>step</code> with the option <code>k = log(n)</code>. In addition we set <code>trace FALSE</code> to avoid verbose printing. The output input of <code>step</code> is our linear model and the output is another linear model that contains the selected variables only.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bicmlr &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Compressive strength</span><span class="st">`</span> <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train_data) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">step</span>(<span class="dt">k =</span> <span class="kw">log</span>(<span class="kw">nrow</span>(train_data)), <span class="dt">trace =</span> <span class="ot">FALSE</span>)
rmse_train_bicmlr &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((y_train <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(bicmlr, train_data))<span class="op">^</span><span class="dv">2</span>))
rmse_test_bicmlr &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((y_test <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(bicmlr, test_data))<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<p>We now repeat the cross validation on the same sets as before</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rmse_cv_bicmlr_k &lt;-<span class="st"> </span><span class="kw">numeric</span>(nfolds)
<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {
  idx &lt;-<span class="st"> </span>folds[[k]]
  mod &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Compressive strength</span><span class="st">`</span> <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train_data[<span class="op">-</span>idx, ]) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">step</span>(<span class="dt">k =</span> <span class="kw">log</span>(<span class="kw">nrow</span>(train_data) <span class="op">-</span><span class="st"> </span><span class="kw">length</span>(idx)), <span class="dt">trace =</span> <span class="ot">FALSE</span>)
  rmse_cv_bicmlr_k[k] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((y_train[idx] <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(mod, train_data[idx, ]))<span class="op">^</span><span class="dv">2</span>))
}
rmse_cv_bicmlr &lt;-<span class="st">  </span><span class="kw">mean</span>(rmse_cv_bicmlr_k)</code></pre></div>
<h4 id="lasso">(3) Lasso</h4>
<p>For the penalized models we now use the <code>glm</code> package, which takes numeric matrices as input. The function <code>glmnet</code> has an elastic mixing input <code>alpha</code>, which does the lasso regression when set to <code>alpha = 1</code>. The Lasso adds a penalty of the form <span class="math inline"><em>λ</em>∥<em>β</em>∥<sub>1</sub></span> to the multiple linear regression model. While we could just “guess” a value for <span class="math inline"><em>λ</em></span>, it is usually obtained from cross validation or from comparing on a test set, this is implemented on the <code>cv.glmnet</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lasso =<span class="st"> </span><span class="kw">cv.glmnet</span>(X_train, y_train, <span class="dt">alpha =</span> <span class="dv">1</span>, <span class="dt">type.measure =</span> <span class="st">&quot;mse&quot;</span>, <span class="dt">nfolds =</span> <span class="dv">10</span>)
rmse_cv_lasso &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">min</span>(lasso<span class="op">$</span>cvm))
rmse_train_lasso &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">predict</span>(lasso, X_train, <span class="dt">s =</span> <span class="st">&quot;lambda.min&quot;</span>) <span class="op">-</span><span class="st"> </span>y_train)<span class="op">^</span><span class="dv">2</span>))
rmse_test_lasso &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">predict</span>(lasso, X_test, <span class="dt">s =</span> <span class="st">&quot;lambda.min&quot;</span>) <span class="op">-</span><span class="st"> </span>y_test)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<h4 id="ridge">(4) Ridge</h4>
<p>Setting <code>alpha = 0</code> the <code>glmnet</code> regression yields Ridge.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ridge =<span class="st"> </span><span class="kw">cv.glmnet</span>(X_train, y_train, <span class="dt">alpha =</span> <span class="dv">0</span>, <span class="dt">type.measure =</span> <span class="st">&quot;mse&quot;</span>, <span class="dt">nfolds =</span> <span class="dv">10</span>)
rmse_cv_ridge &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">min</span>(ridge<span class="op">$</span>cvm))
rmse_train_ridge &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">predict</span>(ridge, X_train, <span class="dt">s =</span> <span class="st">&quot;lambda.min&quot;</span>) <span class="op">-</span><span class="st"> </span>y_train)<span class="op">^</span><span class="dv">2</span>))
rmse_test_ridge &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">predict</span>(ridge, X_test, <span class="dt">s =</span> <span class="st">&quot;lambda.min&quot;</span>) <span class="op">-</span><span class="st"> </span>y_test)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<h4 id="elastic-net">(5) Elastic Net</h4>
<p>An elastic net interpolates the lasso and ridge regularization. By setting <code>alpha = 0.5</code> in the <code>cv.glmnet</code> function we compute an elastic net.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">enet =<span class="st"> </span><span class="kw">cv.glmnet</span>(X_train, y_train, <span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">type.measure =</span> <span class="st">&quot;mse&quot;</span>, <span class="dt">nfolds =</span> <span class="dv">10</span>)
rmse_cv_enet &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">min</span>(enet<span class="op">$</span>cvm))
rmse_train_enet &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">predict</span>(enet, X_train, <span class="dt">s =</span> <span class="st">&quot;lambda.min&quot;</span>) <span class="op">-</span><span class="st"> </span>y_train)<span class="op">^</span><span class="dv">2</span>))
rmse_test_enet &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">predict</span>(enet, X_test, <span class="dt">s =</span> <span class="st">&quot;lambda.min&quot;</span>) <span class="op">-</span><span class="st"> </span>y_test)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<h3 id="comparison">Comparison</h3>
<p>Here is a table with the performance of each model using the Root MSE.</p>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="left">Training</th>
<th align="left">10-Fold CV</th>
<th align="left">Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">MLR</td>
<td align="left">10.3459</td>
<td align="left">10.5037</td>
<td align="left">10.5281</td>
</tr>
<tr class="even">
<td align="left">MLR with BIC</td>
<td align="left">10.4303</td>
<td align="left">10.6091</td>
<td align="left">10.5453</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="left">10.3466</td>
<td align="left">10.5932</td>
<td align="left">10.5206</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="left">10.4959</td>
<td align="left">10.6980</td>
<td align="left">10.4797</td>
</tr>
<tr class="odd">
<td align="left">Elastic Net (alpha=0.5)</td>
<td align="left">10.3467</td>
<td align="left">10.5641</td>
<td align="left">10.5205</td>
</tr>
</tbody>
</table>
<p>From the table we see most models performed equally well on the test set. This is not often the case, but it is plausible since we don’t have a large number of variables, which is the scenario where penalized models usually perform better. In the table it is shown that Ridge is the best performer, but after trying different random seed is the way we splitted the data, we saw that this is not always the case; sometimes even Multiple Linear Regression can be the best one.</p>
<h3 id="appendix-plots">Appendix: Plots</h3>
<p>We add some plots to our analysis to better understand what happened.</p>
<h4 id="a-quality-of-fit">(A) Quality of Fit</h4>
<p>It is always good to compare the predicted vs the fitted values. Since Ridge regression was the best in our test, and most models performed the same, we show the plot for the Ridge model only.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plotdata &lt;-<span class="st"> </span>concrete <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Dataset =</span> <span class="kw">ifelse</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(concrete) <span class="op">%in%</span><span class="st"> </span>train_idx, <span class="st">&quot;Train&quot;</span>, <span class="st">&quot;Test&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Predicted =</span> <span class="kw">as.numeric</span>(<span class="kw">predict</span>(ridge, <span class="kw">as.matrix</span>(concrete[ ,<span class="op">-</span><span class="kw">ncol</span>(concrete)]))))
<span class="kw">ggplot</span>(plotdata, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="st">`</span><span class="dt">Compressive strength</span><span class="st">`</span>, <span class="dt">y =</span> Predicted, <span class="dt">colour =</span> Dataset)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">3</span>, <span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Ridge Regression: Predicted vs Observed&quot;</span>)</code></pre></div>
<p><img src="HW5_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /></p>
<h4 id="b-comparing-the-lambda-in-penalized-regression">(B) Comparing the <span class="math inline"><em>λ</em></span> in Penalized Regression</h4>
<p>When doing models like Lasso, Ridge and Elastic Nets it is a good practice to see how the coefficients shrink as <span class="math inline"><em>λ</em></span> varies, as well as to see the different cross-validated errors</p>
</body>
</html>
